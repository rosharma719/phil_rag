{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructs a nondirected similarity graph. \n",
    "First, we parallel batch upload all nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pinecone import Pinecone\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configurations\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east1-gcp\")\n",
    "PINECONE_INDEX_NAME = \"belief-embeddings\"\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "# Script Constants\n",
    "FETCH_BATCH_SIZE = 500\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "FAILED_UPLOADS_FILE = \"failed_uploads.json\"\n",
    "VECTOR_IDS_FILE = \"../randomly_discovered_ids.txt\"  # Relative path to ensure it finds the file\n",
    "\n",
    "# Toggles\n",
    "RESET_NEO4J = False\n",
    "RESET_LOGS = False\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Logging setup\n",
    "LOG_FILE = \"pipeline.log\"\n",
    "\n",
    "if RESET_LOGS:\n",
    "    # Clear the log file\n",
    "    with open(LOG_FILE, \"w\"):\n",
    "        pass\n",
    "    # Also remove the checkpoint files, if they exist\n",
    "    processed_path = os.path.join(CHECKPOINT_DIR, \"processed_ids.pkl\")\n",
    "    uploaded_path  = os.path.join(CHECKPOINT_DIR, \"uploaded_ids.pkl\")\n",
    "    if os.path.exists(processed_path):\n",
    "        os.remove(processed_path)\n",
    "    if os.path.exists(uploaded_path):\n",
    "        os.remove(uploaded_path)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(LOG_FILE)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"‚úÖ Logging initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "\n",
    "def init_pinecone():\n",
    "    \"\"\"\n",
    "    Initialize Pinecone connection and get index handle.\n",
    "    \"\"\"\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "    index = pc.Index(PINECONE_INDEX_NAME)\n",
    "    stats = index.describe_index_stats()\n",
    "    logger.info(f\"‚úÖ Connected to Pinecone. Total vectors: {stats.get('total_vector_count', 0)}\")\n",
    "    return index, pc\n",
    "\n",
    "def init_neo4j():\n",
    "    \"\"\"Initialize Neo4j connection with retry logic.\"\"\"\n",
    "    if not NEO4J_URI:\n",
    "        raise ValueError(\"‚ö†Ô∏è NEO4J_URI is not set. Check your .env file.\")\n",
    "\n",
    "    max_attempts = 3\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            driver = GraphDatabase.driver(\n",
    "                NEO4J_URI,\n",
    "                auth=(NEO4J_USER, NEO4J_PASSWORD),\n",
    "                max_connection_lifetime=3600\n",
    "            )\n",
    "            with driver.session() as session:\n",
    "                # Quick test query\n",
    "                if session.run(\"RETURN 1\").single():\n",
    "                    logger.info(\"‚úÖ Successfully connected to Neo4j\")\n",
    "                    return driver\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Connection attempt {attempt + 1} failed: {str(e)}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    raise Exception(\"‚ùå Failed to connect to Neo4J after multiple attempts.\")\n",
    "\n",
    "def reset_neo4j(driver):\n",
    "    \"\"\"Delete all nodes and relationships from Neo4j (if enabled).\"\"\"\n",
    "    if RESET_NEO4J:\n",
    "        with driver.session() as session:\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        logger.info(\"‚ùå Neo4j database fully reset.\")\n",
    "\n",
    "\n",
    "# Initialize\n",
    "index, pc = init_pinecone()\n",
    "driver = init_neo4j()\n",
    "\n",
    "# Reset Neo4j if toggled\n",
    "reset_neo4j(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "\n",
    "class ProcessTracker:\n",
    "    def __init__(self):\n",
    "        # We'll load any existing (if logs aren't reset)\n",
    "        self.processed_ids = set()\n",
    "        self.uploaded_ids = set()\n",
    "        self.load_checkpoint()\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load processed and uploaded IDs from disk.\"\"\"\n",
    "        processed_path = os.path.join(CHECKPOINT_DIR, \"processed_ids.pkl\")\n",
    "        uploaded_path  = os.path.join(CHECKPOINT_DIR, \"uploaded_ids.pkl\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(processed_path):\n",
    "                with open(processed_path, \"rb\") as f:\n",
    "                    self.processed_ids = pickle.load(f)\n",
    "                logger.info(f\"üîÑ Loaded {len(self.processed_ids)} processed IDs.\")\n",
    "            if os.path.exists(uploaded_path):\n",
    "                with open(uploaded_path, \"rb\") as f:\n",
    "                    self.uploaded_ids = pickle.load(f)\n",
    "                logger.info(f\"üîÑ Loaded {len(self.uploaded_ids)} uploaded IDs.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ö†Ô∏è Error loading checkpoints: {str(e)}\")\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save progress to disk.\"\"\"\n",
    "        processed_path = os.path.join(CHECKPOINT_DIR, \"processed_ids.pkl\")\n",
    "        uploaded_path  = os.path.join(CHECKPOINT_DIR, \"uploaded_ids.pkl\")\n",
    "        try:\n",
    "            with open(processed_path, \"wb\") as f:\n",
    "                pickle.dump(self.processed_ids, f)\n",
    "            with open(uploaded_path, \"wb\") as f:\n",
    "                pickle.dump(self.uploaded_ids, f)\n",
    "            logger.info(f\"‚úÖ Saved checkpoints: {len(self.processed_ids)} processed, {len(self.uploaded_ids)} uploaded\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ö†Ô∏è Error saving checkpoints: {str(e)}\")\n",
    "\n",
    "    def mark_processed(self, vec_ids):\n",
    "        self.processed_ids.update(vec_ids)\n",
    "        self.save_checkpoint()\n",
    "\n",
    "    def mark_uploaded(self, vec_ids):\n",
    "        self.uploaded_ids.update(vec_ids)\n",
    "        self.save_checkpoint()\n",
    "\n",
    "    def get_unprocessed(self, all_ids):\n",
    "        return [vid for vid in all_ids if vid not in self.processed_ids]\n",
    "\n",
    "    def get_unuploaded(self, all_ids):\n",
    "        return [vid for vid in all_ids if vid not in self.uploaded_ids]\n",
    "\n",
    "\n",
    "tracker = ProcessTracker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "\n",
    "def fetch_vector_batch(index, batch_ids):\n",
    "    \"\"\"\n",
    "    Fetch metadata from Pinecone for given 'batch_ids'.\n",
    "    \"\"\"\n",
    "    if not batch_ids:\n",
    "        return []\n",
    "    try:\n",
    "        response = index.fetch(ids=batch_ids)\n",
    "        return [\n",
    "            {\"id\": vid, \"metadata\": vdata.metadata}\n",
    "            for vid, vdata in response.vectors.items()\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error fetching vectors: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def upload_to_neo4j(driver, vectors):\n",
    "    \"\"\"\n",
    "    Batch upload metadata to Neo4j.\n",
    "    \"\"\"\n",
    "    if not vectors:\n",
    "        return\n",
    "\n",
    "    # Filter out any vectors already uploaded\n",
    "    unuploaded_vectors = [v for v in vectors if v[\"id\"] not in tracker.uploaded_ids]\n",
    "    if not unuploaded_vectors:\n",
    "        return\n",
    "\n",
    "    BATCH_UPLOAD_SIZE = 500\n",
    "    failed_uploads = []\n",
    "    success_ids = []\n",
    "\n",
    "    with driver.session() as session:\n",
    "        for i in range(0, len(unuploaded_vectors), BATCH_UPLOAD_SIZE):\n",
    "            batch = unuploaded_vectors[i : i + BATCH_UPLOAD_SIZE]\n",
    "\n",
    "            concept_batch = [v for v in batch if v[\"metadata\"].get(\"type\", \"\").lower() == \"concept\"]\n",
    "            belief_batch  = [v for v in batch if v[\"metadata\"].get(\"type\", \"\").lower() == \"belief\"]\n",
    "            unknown_batch = [v for v in batch if v[\"metadata\"].get(\"type\", \"\").lower() not in (\"concept\",\"belief\")]\n",
    "\n",
    "            try:\n",
    "                if concept_batch:\n",
    "                    session.execute_write(lambda tx: tx.run(\n",
    "                        \"\"\"\n",
    "                        UNWIND $concepts AS item\n",
    "                        MERGE (n:Concept { id: item.id })\n",
    "                        SET n += item.metadata\n",
    "                        \"\"\",\n",
    "                        concepts=concept_batch\n",
    "                    ))\n",
    "                if belief_batch:\n",
    "                    session.execute_write(lambda tx: tx.run(\n",
    "                        \"\"\"\n",
    "                        UNWIND $beliefs AS item\n",
    "                        MERGE (n:Belief { id: item.id })\n",
    "                        SET n += item.metadata\n",
    "                        \"\"\",\n",
    "                        beliefs=belief_batch\n",
    "                    ))\n",
    "                if unknown_batch:\n",
    "                    session.execute_write(lambda tx: tx.run(\n",
    "                        \"\"\"\n",
    "                        UNWIND $unknowns AS item\n",
    "                        MERGE (n:UnknownType { id: item.id })\n",
    "                        SET n += item.metadata\n",
    "                        \"\"\",\n",
    "                        unknowns=unknown_batch\n",
    "                    ))\n",
    "\n",
    "                # Mark success\n",
    "                success_ids.extend([v[\"id\"] for v in batch])\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error writing batch to Neo4j: {str(e)}\")\n",
    "                failed_uploads.extend([v[\"id\"] for v in batch])\n",
    "\n",
    "    if success_ids:\n",
    "        tracker.mark_uploaded(success_ids)\n",
    "\n",
    "    if failed_uploads:\n",
    "        with open(FAILED_UPLOADS_FILE, \"w\") as f:\n",
    "            json.dump(failed_uploads, f, indent=2)\n",
    "        logger.error(f\"‚ùå Failed uploads logged in {FAILED_UPLOADS_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "\n",
    "async def main():\n",
    "    # 1) Read all vector IDs from randomly_discovered_ids.txt\n",
    "    with open(VECTOR_IDS_FILE, \"r\") as f:\n",
    "        all_ids = f.read().splitlines()\n",
    "    logger.info(f\"üìå Found {len(all_ids)} IDs in '{VECTOR_IDS_FILE}'\")\n",
    "\n",
    "    # 2) Filter out already processed ones\n",
    "    unprocessed_ids = tracker.get_unprocessed(all_ids)\n",
    "    logger.info(f\"üîé {len(unprocessed_ids)} unprocessed IDs remain.\")\n",
    "\n",
    "    # 3) Process in batches\n",
    "    for i in range(0, len(unprocessed_ids), FETCH_BATCH_SIZE):\n",
    "        batch_ids = unprocessed_ids[i : i + FETCH_BATCH_SIZE]\n",
    "        # Fetch from Pinecone\n",
    "        vectors = fetch_vector_batch(index, batch_ids)\n",
    "        # Mark as processed\n",
    "        tracker.mark_processed(batch_ids)\n",
    "        # Upload to Neo4j\n",
    "        upload_to_neo4j(driver, vectors)\n",
    "\n",
    "        batch_num = (i // FETCH_BATCH_SIZE) + 1\n",
    "        logger.info(f\"‚úÖ Finished batch {batch_num} of size {len(batch_ids)}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_ids = pickle.load(open(\"vector_ids.pkl\", \"rb\"))\n",
    "unique_ids = set(all_ids)\n",
    "print(len(all_ids), len(unique_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
