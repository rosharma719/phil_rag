{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since concepts and beliefs were extracted from documents, there are duplicate Concept nodes, but with separate origins and meanings.\n",
    "We're going to restructure the nodes to combine synonymous nodes into a new class apart from Belief and Concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Neo4j Config\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "# Pinecone setup\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east1-gcp\")\n",
    "PINECONE_INDEX_NAME = \"belief-embeddings\"\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "# Script Constants\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "RESET_OVERLOADED_NODES = False\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Logging setup\n",
    "LOG_FILE = \"concept_processing.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(LOG_FILE)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"âœ… Logging initialized.\")\n",
    "\n",
    "# Initialize Neo4j Connection\n",
    "def init_neo4j():\n",
    "    if not NEO4J_URI:\n",
    "        raise ValueError(\"âš ï¸ NEO4J_URI is not set. Check your .env file.\")\n",
    "    \n",
    "    driver = GraphDatabase.driver(\n",
    "        NEO4J_URI,\n",
    "        auth=(NEO4J_USER, NEO4J_PASSWORD),\n",
    "        max_connection_lifetime=3600\n",
    "    )\n",
    "    with driver.session() as session:\n",
    "        if session.run(\"RETURN 1\").single():\n",
    "            logger.info(\"âœ… Successfully connected to Neo4j\")\n",
    "            return driver\n",
    "    raise Exception(\"âŒ Failed to connect to Neo4J.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_overloaded_nodes(driver):\n",
    "    if RESET_OVERLOADED_NODES:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(\"MATCH (oc:Overloaded_Concept) DETACH DELETE oc RETURN count(*) as deleted\")\n",
    "            deleted = result.single()[\"deleted\"]\n",
    "            logger.info(f\"âŒ Deleted {deleted} Overloaded_Concept nodes.\")\n",
    "\n",
    "class ProcessTracker:\n",
    "    def __init__(self):\n",
    "        self.processed_ids = set()\n",
    "\n",
    "    def mark_processed(self, vec_ids):\n",
    "        if vec_ids:\n",
    "            self.processed_ids.update(vec_ids)\n",
    "\n",
    "def find_duplicate_concepts(driver):\n",
    "    duplicate_concepts = {}\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Concept)\n",
    "            WITH c.concept as concept_name, collect(c) as concepts\n",
    "            WHERE size(concepts) > 1\n",
    "            RETURN concept_name, [c IN concepts | {\n",
    "                id: c.id, \n",
    "                sep_id: c.sep_id, \n",
    "                document_title: c.document_title\n",
    "            }] as concept_instances\n",
    "            \"\"\"\n",
    "        )\n",
    "        for record in result:\n",
    "            concept_name = record[\"concept_name\"]\n",
    "            instances = record[\"concept_instances\"]\n",
    "            duplicate_concepts[concept_name] = instances\n",
    "    logger.info(f\"ðŸ” Found {len(duplicate_concepts)} concepts with duplicates\")\n",
    "    return duplicate_concepts\n",
    "\n",
    "def merge_duplicate_concepts(driver, duplicate_concepts, tracker):\n",
    "    total_merged = 0\n",
    "    processed_ids = set()\n",
    "    with tqdm(total=len(duplicate_concepts), desc=\"Merging duplicate concepts\") as pbar:\n",
    "        with driver.session() as session:\n",
    "            for concept_name, instances in duplicate_concepts.items():\n",
    "                try:\n",
    "                    ids = [instance[\"id\"] for instance in instances]\n",
    "                    sep_ids = [instance[\"sep_id\"] for instance in instances]\n",
    "                    doc_titles = [instance[\"document_title\"] for instance in instances]\n",
    "                    session.run(\n",
    "                        \"\"\"\n",
    "                        MERGE (oc:Overloaded_Concept {name: $name})\n",
    "                        SET oc.type = \"overloaded_concept\",\n",
    "                            oc.sep_ids = $sep_ids,\n",
    "                            oc.document_titles = $doc_titles,\n",
    "                            oc.vector_ids = $vector_ids\n",
    "                        \"\"\",\n",
    "                        name=concept_name,\n",
    "                        sep_ids=sep_ids,\n",
    "                        doc_titles=doc_titles,\n",
    "                        vector_ids=ids\n",
    "                    )\n",
    "                    delete_result = session.run(\n",
    "                        \"\"\"\n",
    "                        MATCH (c:Concept) \n",
    "                        WHERE c.id IN $ids\n",
    "                        DETACH DELETE c\n",
    "                        RETURN count(*) as deleted\n",
    "                        \"\"\",\n",
    "                        ids=ids\n",
    "                    )\n",
    "                    deleted = delete_result.single()[\"deleted\"]\n",
    "                    logger.info(f\"ðŸ”„ Merged '{concept_name}' from {len(ids)} concepts, deleted {deleted} nodes\")\n",
    "                    processed_ids.update(ids)\n",
    "                    total_merged += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"âŒ Error processing '{concept_name}': {str(e)}\")\n",
    "                pbar.update(1)\n",
    "    tracker.mark_processed(list(processed_ids))\n",
    "    logger.info(f\"âœ… Successfully merged {total_merged} duplicate concepts\")\n",
    "    return total_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    driver = init_neo4j()\n",
    "    reset_overloaded_nodes(driver)\n",
    "    tracker = ProcessTracker()\n",
    "    try:\n",
    "        logger.info(\"ðŸ” Searching for duplicate concepts across the database...\")\n",
    "        duplicate_concepts = find_duplicate_concepts(driver)\n",
    "        if duplicate_concepts:\n",
    "            merge_duplicate_concepts(driver, duplicate_concepts, tracker)\n",
    "        else:\n",
    "            logger.info(\"â„¹ï¸ No duplicate concepts found in the database\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Unexpected error: {str(e)}\")\n",
    "    finally:\n",
    "        driver.close()\n",
    "        logger.info(\"âœ… Script execution completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do the same thing for Pinecone, and attach that to this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Configurations\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pinecone import Pinecone\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Pinecone Config\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east1-gcp\")\n",
    "PINECONE_INDEX_NAME = \"belief-embeddings\"\n",
    "\n",
    "# Neo4j Config\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "# Script Constants\n",
    "FETCH_BATCH_SIZE = 100\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "FAILED_UPLOADS_FILE = \"failed_uploads.json\"\n",
    "\n",
    "# Toggles\n",
    "RESET_PROCESSED_IDS = False  # Toggle to reset the processed IDs file\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Logging setup\n",
    "LOG_FILE = \"pinecone_overloaded_processing.log\"\n",
    "\n",
    "if RESET_PROCESSED_IDS:\n",
    "    processed_path = os.path.join(CHECKPOINT_DIR, \"processed_ids.pkl\")\n",
    "    if os.path.exists(processed_path):\n",
    "        os.remove(processed_path)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(LOG_FILE)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"âœ… Logging initialized.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: Initialize Pinecone & Neo4j Connections\n",
    "\n",
    "def init_pinecone():\n",
    "    \"\"\"\n",
    "    Initialize Pinecone connection and get index handle.\n",
    "    \"\"\"\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "    index = pc.Index(PINECONE_INDEX_NAME)\n",
    "    stats = index.describe_index_stats()\n",
    "    logger.info(f\"âœ… Connected to Pinecone. Total vectors: {stats.get('total_vector_count', 0)}\")\n",
    "    return index, pc\n",
    "\n",
    "def init_neo4j():\n",
    "    \"\"\"Initialize Neo4j connection with retry logic.\"\"\"\n",
    "    if not NEO4J_URI:\n",
    "        raise ValueError(\"âš ï¸ NEO4J_URI is not set. Check your .env file.\")\n",
    "\n",
    "    max_attempts = 3\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            driver = GraphDatabase.driver(\n",
    "                NEO4J_URI,\n",
    "                auth=(NEO4J_USER, NEO4J_PASSWORD),\n",
    "                max_connection_lifetime=3600\n",
    "            )\n",
    "            with driver.session() as session:\n",
    "                if session.run(\"RETURN 1\").single():\n",
    "                    logger.info(\"âœ… Successfully connected to Neo4j\")\n",
    "                    return driver\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"âš ï¸ Connection attempt {attempt + 1} failed: {str(e)}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    \n",
    "    raise Exception(\"âŒ Failed to connect to Neo4J after multiple attempts.\")\n",
    "\n",
    "index, pc = init_pinecone()\n",
    "driver = init_neo4j()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 3: Process Tracker (Checkpoints)\n",
    "\n",
    "class ProcessTracker:\n",
    "    def __init__(self):\n",
    "        self.processed_ids = set()\n",
    "        self.failed_ids = set()\n",
    "        self.load_checkpoint()\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load processed IDs from disk unless reset is enabled.\"\"\"\n",
    "        processed_path = os.path.join(CHECKPOINT_DIR, \"processed_ids.pkl\")\n",
    "\n",
    "        if not RESET_PROCESSED_IDS and os.path.exists(processed_path):\n",
    "            try:\n",
    "                with open(processed_path, \"rb\") as f:\n",
    "                    self.processed_ids = pickle.load(f)\n",
    "                logger.info(f\"ðŸ”„ Loaded {len(self.processed_ids)} processed IDs.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âš ï¸ Error loading processed IDs: {str(e)}\")\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save processed IDs to disk.\"\"\"\n",
    "        processed_path = os.path.join(CHECKPOINT_DIR, \"processed_ids.pkl\")\n",
    "        try:\n",
    "            with open(processed_path, \"wb\") as f:\n",
    "                pickle.dump(self.processed_ids, f)\n",
    "            logger.info(f\"âœ… Saved {len(self.processed_ids)} processed IDs.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âš ï¸ Error saving processed IDs: {str(e)}\")\n",
    "\n",
    "    def mark_processed(self, vec_ids):\n",
    "        self.processed_ids.update(vec_ids)\n",
    "        self.save_checkpoint()\n",
    "\n",
    "    def get_unprocessed(self, all_concepts, resume_fraction=0.95):\n",
    "        \"\"\"\n",
    "        Returns only unprocessed overloaded concepts.\n",
    "        Starts processing from `resume_fraction` to avoid reprocessing.\n",
    "        \"\"\"\n",
    "        start_index = int(len(all_concepts) * resume_fraction)  # Resume 5/6 through\n",
    "        return [\n",
    "            concept for concept in all_concepts[start_index:]\n",
    "            if not all(vid in self.processed_ids for vid in concept[\"vector_ids\"])\n",
    "        ]\n",
    "\n",
    "\n",
    "tracker = ProcessTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Fetch Overloaded Concepts from Neo4j\n",
    "\n",
    "def fetch_overloaded_concepts():\n",
    "    \"\"\"\n",
    "    Fetch all Overloaded_Concept nodes from Neo4j.\n",
    "    \"\"\"\n",
    "    overloaded_concepts = []\n",
    "    with driver.session() as session:\n",
    "        results = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (oc:Overloaded_Concept)\n",
    "            RETURN oc.name AS name, oc.sep_ids AS sep_ids, oc.document_titles AS document_titles, oc.vector_ids AS vector_ids\n",
    "            \"\"\"\n",
    "        )\n",
    "        for record in results:\n",
    "            overloaded_concepts.append({\n",
    "                \"name\": record[\"name\"],\n",
    "                \"sep_ids\": record[\"sep_ids\"],\n",
    "                \"document_titles\": record[\"document_titles\"],\n",
    "                \"vector_ids\": record[\"vector_ids\"]\n",
    "            })\n",
    "    \n",
    "    return overloaded_concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# Cell 5: Process Overloaded Concepts in Pinecone\n",
    "\n",
    "def process_overloaded_concepts(overloaded_concepts):\n",
    "    \"\"\"\n",
    "    Process Overloaded_Concepts by selecting a master vector, creating a new merged vector,\n",
    "    and deleting old redundant vectors.\n",
    "    \"\"\"\n",
    "    if not overloaded_concepts:\n",
    "        return\n",
    "\n",
    "    failed_ids = []\n",
    "    batch_size = 100\n",
    "    batch_counter = 0\n",
    "\n",
    "    for i in range(0, len(overloaded_concepts), batch_size):\n",
    "        batch = overloaded_concepts[i : i + batch_size]\n",
    "        new_vectors = []\n",
    "        delete_vectors = []\n",
    "\n",
    "        for concept in batch:\n",
    "            vector_ids = concept[\"vector_ids\"]\n",
    "            sep_ids = concept[\"sep_ids\"]\n",
    "            document_titles = concept[\"document_titles\"]\n",
    "            name = concept[\"name\"]\n",
    "\n",
    "            # Fetch existing vectors from Pinecone\n",
    "            response = index.fetch(vector_ids)\n",
    "            valid_vectors = {vid: vdata for vid, vdata in response.vectors.items()}\n",
    "\n",
    "            if not valid_vectors:\n",
    "                logger.warning(f\"âš ï¸ No valid vectors found for Overloaded_Concept: {name}\")\n",
    "                continue\n",
    "\n",
    "            # Select the first vector as the master vector\n",
    "            master_vector_id, master_vector_data = next(iter(valid_vectors.items()))\n",
    "            master_embedding = master_vector_data.values  # Extract embedding from the master\n",
    "\n",
    "            # Create new overloaded vector ID\n",
    "            new_vector_id = f\"{sep_ids[0]}_overloaded_concept_{hash(name) % 1000000}\"\n",
    "\n",
    "            new_metadata = {\n",
    "                \"concept\": name,\n",
    "                \"sep_ids\": [str(int(s)) for s in sep_ids],  # Convert to list of strings\n",
    "                \"document_titles\": document_titles,\n",
    "                \"original_vector_ids\": vector_ids\n",
    "            }\n",
    "\n",
    "\n",
    "            # Ensure master_embedding is a NumPy array before calling .tolist()\n",
    "            if isinstance(master_embedding, list):\n",
    "                master_embedding = np.array(master_embedding)  # Convert to NumPy array\n",
    "\n",
    "            new_vectors.append((new_vector_id, master_embedding.tolist(), new_metadata))\n",
    "\n",
    "            # Mark old vectors for deletion (only delete after successful upload)\n",
    "            delete_vectors.extend(valid_vectors.keys())\n",
    "\n",
    "        # Upload new vectors in batch\n",
    "        if new_vectors:\n",
    "            index.upsert(vectors=new_vectors)\n",
    "            logger.info(f\"âœ… Uploaded {len(new_vectors)} new overloaded vectors.\")\n",
    "        # Delete old vectors in batches of 1000\n",
    "        if delete_vectors:\n",
    "            batch_size = 1000\n",
    "            for j in range(0, len(delete_vectors), batch_size):\n",
    "                batch_delete = delete_vectors[j : j + batch_size]\n",
    "                index.delete(ids=batch_delete)\n",
    "                logger.info(f\"âŒ Deleted {len(batch_delete)} redundant vectors. Total so far: {j + len(batch_delete)}/{len(delete_vectors)}\")\n",
    "\n",
    "\n",
    "\n",
    "        batch_counter += 1\n",
    "        if batch_counter % 2 == 0:  # Log every 200 concepts\n",
    "            logger.info(f\"ðŸ“¢ Processed {batch_counter * batch_size} overloaded concepts so far.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6: Main Execution\n",
    "\n",
    "def main():\n",
    "    overloaded_concepts = fetch_overloaded_concepts()\n",
    "    logger.info(f\"ðŸ“Œ Found {len(overloaded_concepts)} overloaded concepts in Neo4j.\")\n",
    "\n",
    "    unprocessed_concepts = tracker.get_unprocessed(overloaded_concepts, resume_fraction=5/6)\n",
    "    logger.info(f\"ðŸ”Ž {len(unprocessed_concepts)} overloaded concepts remain unprocessed.\")\n",
    "\n",
    "    process_overloaded_concepts(unprocessed_concepts)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(vector=[0.1] * 768, top_k=1, include_metadata=True)\n",
    "\n",
    "if response.matches:\n",
    "    print(\"Found overloaded concept vector:\", response.matches[0].id)\n",
    "else:\n",
    "    print(\"No overloaded vectors found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some debug stuff. I need to get rid of some remaining redundant Pinecone vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Pinecone Index (assuming `index` is already initialized)\n",
    "VECTOR_IDS_FILE = \"vector_ids.pkl\"\n",
    "\n",
    "def fetch_and_save_vector_ids():\n",
    "    \"\"\"Fetch all vector IDs from Pinecone and save them.\"\"\"\n",
    "    logger.info(\"ðŸ” Fetching all vector IDs from Pinecone...\")\n",
    "\n",
    "    # Get total number of vectors\n",
    "    response = index.describe_index_stats()\n",
    "    total_vectors = response[\"total_vector_count\"]\n",
    "    logger.info(f\"ðŸ“Œ Total vectors in Pinecone: {total_vectors}\")\n",
    "\n",
    "    # Fetch and flatten list of vector IDs\n",
    "    raw_vector_ids = list(index.list())  # This may return nested lists\n",
    "    vector_ids = [vid for sublist in raw_vector_ids for vid in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "    logger.info(f\"âœ… Retrieved {len(vector_ids)} vector IDs.\")\n",
    "\n",
    "    # Save to disk\n",
    "    with open(VECTOR_IDS_FILE, \"wb\") as f:\n",
    "        pickle.dump(vector_ids, f)\n",
    "    logger.info(f\"ðŸ’¾ Saved vector IDs to {VECTOR_IDS_FILE}.\")\n",
    "\n",
    "# Run first cell\n",
    "fetch_and_save_vector_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Pinecone Index (assuming `index` is already initialized)\n",
    "CHECKPOINT_FILE = \"processed_vector_ids.pkl\"\n",
    "VECTOR_IDS_FILE = \"vector_ids.pkl\"\n",
    "BATCH_SIZE = 500  # ðŸ”§ Reduced batch size to avoid URI limits\n",
    "\n",
    "# Load vector IDs from file\n",
    "def load_vector_ids():\n",
    "    \"\"\"Load saved vector IDs from file.\"\"\"\n",
    "    with open(VECTOR_IDS_FILE, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Load or initialize processed vector IDs\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load previously processed vector IDs to resume without duplication.\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return set()\n",
    "\n",
    "def save_checkpoint(processed_vectors):\n",
    "    \"\"\"Save processed vector IDs.\"\"\"\n",
    "    with open(CHECKPOINT_FILE, \"wb\") as f:\n",
    "        pickle.dump(processed_vectors, f)\n",
    "\n",
    "def count_redundant_vector_ids():\n",
    "    \"\"\"\n",
    "    Identify and count redundant vector IDs in Pinecone.\n",
    "    \"\"\"\n",
    "    vector_counts = defaultdict(int)  # Track occurrences of each vector ID\n",
    "    processed_vectors = load_checkpoint()  # Load already processed IDs\n",
    "    all_vector_ids = load_vector_ids()  # Load saved vector IDs\n",
    "\n",
    "    logger.info(f\"ðŸ”Ž Total vector IDs: {len(all_vector_ids)}\")\n",
    "\n",
    "    # Filter out already processed vectors\n",
    "    unprocessed_vector_ids = [vid for vid in all_vector_ids if vid not in processed_vectors]\n",
    "    logger.info(f\"ðŸ”Ž {len(unprocessed_vector_ids)} vector IDs left to process.\")\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(unprocessed_vector_ids), BATCH_SIZE):\n",
    "        batch_ids = unprocessed_vector_ids[i : i + BATCH_SIZE]\n",
    "\n",
    "        try:\n",
    "            response = index.fetch(ids=batch_ids)  # Fetch metadata only\n",
    "            for vector_id in response.vectors.keys():\n",
    "                vector_counts[vector_id] += 1  # Count occurrences of each vector ID\n",
    "\n",
    "            # Save progress\n",
    "            processed_vectors.update(batch_ids)\n",
    "            save_checkpoint(processed_vectors)\n",
    "\n",
    "            logger.info(f\"âœ… Processed {i + len(batch_ids)}/{len(unprocessed_vector_ids)} vector IDs...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error processing batch {i//BATCH_SIZE + 1}: {str(e)}\")\n",
    "            time.sleep(2)  # Pause before retrying\n",
    "\n",
    "    # Identify redundant vector IDs (appearing more than once)\n",
    "    redundant_vector_ids = {k: v for k, v in vector_counts.items() if v > 1}\n",
    "    logger.info(f\"ðŸ”Ž Found {len(redundant_vector_ids)} redundant vector IDs.\")\n",
    "\n",
    "    return redundant_vector_ids\n",
    "\n",
    "# Run second cell\n",
    "redundant_vector_ids = count_redundant_vector_ids()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
