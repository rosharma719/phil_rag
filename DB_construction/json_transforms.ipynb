{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code DOES NOT replace rows when run. \n",
    "Picks up where left off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# PostgreSQL Connection Settings\n",
    "DB_NAME = \"phil_rag\"\n",
    "DB_USER = \"rohansharma\"\n",
    "DB_HOST = \"localhost\"\n",
    "\n",
    "# Directory containing JSON files\n",
    "JSON_DIRECTORY = \"/Volumes/BigDrive/phil_rag\"\n",
    "\n",
    "# Load Embedding Model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(dbname=DB_NAME, user=DB_USER, host=DB_HOST)\n",
    "cur = conn.cursor()\n",
    "print(\"✅ Connected to PostgreSQL\")\n",
    "\n",
    "# Get last processed title\n",
    "cur.execute(\"SELECT title FROM sep_embeddings ORDER BY id DESC LIMIT 1;\")\n",
    "last_processed = cur.fetchone()\n",
    "last_processed_title = last_processed[0] if last_processed else None\n",
    "\n",
    "# Delete incomplete data for the last title (ensures clean restart)\n",
    "if last_processed_title:\n",
    "    print(f\"🗑️ Removing incomplete entries for: {last_processed_title}\")\n",
    "    cur.execute(\"DELETE FROM sep_embeddings WHERE title = %s;\", (last_processed_title,))\n",
    "    conn.commit()\n",
    "\n",
    "# Insert function with deletion handling\n",
    "insert_count = 0\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def insert_into_db(title, section, content, embedding):\n",
    "    global insert_count\n",
    "    content_hash = hashlib.md5((title + section + content).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    sql = \"\"\"\n",
    "    INSERT INTO sep_embeddings (title, section, content, embedding, hash)\n",
    "    VALUES (%s, %s, %s, %s, %s)\n",
    "    ON CONFLICT (hash) DO NOTHING;\n",
    "    \"\"\"  \n",
    "    cur.execute(sql, (title, section, content, embedding, content_hash))\n",
    "    insert_count += 1\n",
    "\n",
    "    if insert_count % BATCH_SIZE == 0:\n",
    "        conn.commit()\n",
    "        print(f\"🔄 Committed {insert_count} inserts so far...\")\n",
    "\n",
    "# Get JSON files\n",
    "json_files = [f for f in os.listdir(JSON_DIRECTORY) if f.endswith(\".json\")]\n",
    "total_files = len(json_files)\n",
    "\n",
    "print(f\"📂 Found {total_files} JSON files. Resuming processing from: {last_processed_title if last_processed_title else 'Start'}\")\n",
    "\n",
    "# Process JSON Files\n",
    "resume = False if last_processed_title else True  # Resume only after the last processed file\n",
    "\n",
    "for filename in tqdm(json_files, desc=\"📊 Processing JSON files\", unit=\"file\"):\n",
    "    file_path = os.path.join(JSON_DIRECTORY, filename)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    title = data.get(\"title\", \"Unknown Title\")  \n",
    "\n",
    "    # Skip already processed files\n",
    "    if not resume:\n",
    "        if title == last_processed_title:\n",
    "            resume = True  # Found the last processed file, resume now\n",
    "        continue  # Skip until we reach the last processed file\n",
    "\n",
    "    sections = data.get(\"sections\", {})\n",
    "\n",
    "    print(f\"\\n📜 Processing Title: {title} with {len(sections)} sections\")\n",
    "\n",
    "    for section_name, section_text in tqdm(\n",
    "        sections.items(), desc=f\"⚡ Processing Sections ({filename})\", leave=False, unit=\"section\"\n",
    "    ):\n",
    "        if len(section_text.strip()) > 0:\n",
    "            embedding = model.encode(section_text).tolist()\n",
    "            insert_into_db(title, section_name, section_text, embedding)\n",
    "\n",
    "# Final commit after last batch\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"🎉 ✅ Vectorization and storage complete! Total inserted: {insert_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
