{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neon was retaining WAL and denying edit perms because its excess WAL was taking up too much storage, so I had to download the pgdump. \n",
    "This script reuploads it.\n",
    "\n",
    "First step: Parse backup dump into JSON and upload in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for backup.sql at: /Users/rohansharma/Desktop/Code/phil_rag/backup.sql\n",
      "✅ backup.sql found, proceeding with conversion...\n",
      "✅ JSON batch conversion complete: 0 batches written.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Get the correct path to backup.sql in the root folder\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Move one level up\n",
    "SQL_FILE = os.path.join(BASE_DIR, \"backup.sql\")\n",
    "JSONL_FILE = os.path.join(BASE_DIR, \"backup.jsonl\")\n",
    "\n",
    "print(f\"Looking for backup.sql at: {SQL_FILE}\")  # Debugging line\n",
    "\n",
    "# Check if the file exists before proceeding\n",
    "if not os.path.exists(SQL_FILE):\n",
    "    raise FileNotFoundError(f\"❌ Error: {SQL_FILE} not found. Are you running the script from the correct directory?\")\n",
    "\n",
    "print(\"✅ backup.sql found, proceeding with conversion...\")\n",
    "\n",
    "def stream_sql_to_json(input_sql, output_jsonl, batch_size=5000):\n",
    "    \"\"\"\n",
    "    Converts SQL INSERT statements to JSONL in batches.\n",
    "    \"\"\"\n",
    "    batch_count = 0\n",
    "    batch = []  # ✅ Ensure batch is always initialized\n",
    "\n",
    "    with open(input_sql, \"r\") as f, open(output_jsonl, \"w\") as f_out:\n",
    "        sql_data = f.read()\n",
    "\n",
    "        # Extract INSERT statements\n",
    "        insert_statements = re.findall(r\"INSERT INTO (\\w+) VALUES (.*?);\", sql_data, re.DOTALL)\n",
    "\n",
    "        for table, values in insert_statements:\n",
    "            values = values.split(\"),(\")\n",
    "            values = [v.strip(\"()\") for v in values]\n",
    "\n",
    "            for row in values:\n",
    "                row_values = row.split(\",\")  # Assuming simple CSV structure\n",
    "                batch.append(row_values)\n",
    "\n",
    "                # Write in batches\n",
    "                if len(batch) >= batch_size:\n",
    "                    json.dump({table: batch}, f_out)\n",
    "                    f_out.write(\"\\n\")\n",
    "                    batch = []  # ✅ Reset batch correctly\n",
    "                    batch_count += 1\n",
    "\n",
    "        # ✅ Final write for any remaining data\n",
    "        if batch:\n",
    "            json.dump({table: batch}, f_out)\n",
    "            batch_count += 1\n",
    "\n",
    "    print(f\"✅ JSON batch conversion complete: {batch_count} batches written.\")\n",
    "\n",
    "# Run conversion\n",
    "stream_sql_to_json(SQL_FILE, JSONL_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
