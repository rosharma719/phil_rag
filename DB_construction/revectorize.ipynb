{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize fulltext and upload to Neon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ğŸ”¥ Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# âœ… Retrieve credentials\n",
    "dbname = os.getenv(\"NEON_DBNAME\", \"neondb\")\n",
    "user = os.getenv(\"NEON_USER\", \"neondb_owner\")\n",
    "password = os.getenv(\"NEON_PASSWORD\")\n",
    "host = os.getenv(\"NEON_HOST\", \"ep-morning-bonus-a8s0mhpb-pooler.eastus2.azure.neon.tech\")\n",
    "port = os.getenv(\"NEON_PORT\", \"5432\")\n",
    "\n",
    "# ğŸ”¥ Encode password (if necessary)\n",
    "encoded_password = quote_plus(password)\n",
    "\n",
    "# âœ… Construct connection string\n",
    "conn_str = f\"postgresql://{user}:{encoded_password}@{host}:{port}/{dbname}?sslmode=require\"\n",
    "\n",
    "# âœ… Load embedding model (Choose BGE or E5)\n",
    "MODEL_NAME = \"BAAI/bge-base-en\"  # Change to \"intfloat/e5-large-v2\" if you prefer E5\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# âœ… Batch size for commits\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# âœ… Connect to PostgreSQL\n",
    "try:\n",
    "    conn = psycopg2.connect(conn_str)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"âœ… Connected to PostgreSQL!\")\n",
    "\n",
    "    # 1ï¸âƒ£ Fetch only entries where embedding is NULL\n",
    "    cursor.execute(\"SELECT id, content FROM sep_embeddings WHERE embedding IS NULL;\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    total_rows = len(rows)\n",
    "    if total_rows == 0:\n",
    "        print(\"ğŸ‰ All embeddings are already computed. No updates needed.\")\n",
    "    else:\n",
    "        print(f\"âš™ï¸ Computing embeddings for {total_rows} new entries...\")\n",
    "\n",
    "        # 2ï¸âƒ£ Compute embeddings in batches\n",
    "        batch_count = 0\n",
    "        for index, (text_id, content) in enumerate(rows, start=1):\n",
    "            if content:  # Avoid empty content\n",
    "                embedding = model.encode(content).tolist()  # Convert numpy array to list\n",
    "\n",
    "                # âœ… Ensure PostgreSQL vector format (if using pgvector)\n",
    "                embedding_str = \"[\" + \",\".join(map(str, embedding)) + \"]\"\n",
    "\n",
    "                # Queue update\n",
    "                cursor.execute(\n",
    "                    \"UPDATE sep_embeddings SET embedding = %s WHERE id = %s;\",\n",
    "                    (embedding_str, text_id)\n",
    "                )\n",
    "\n",
    "                # Commit every `BATCH_SIZE` rows\n",
    "                if index % BATCH_SIZE == 0:\n",
    "                    conn.commit()\n",
    "                    batch_count += 1\n",
    "                    print(f\"âœ… Committed batch {batch_count} ({index}/{total_rows} processed)\")\n",
    "\n",
    "        # 4ï¸âƒ£ Final commit for remaining updates\n",
    "        conn.commit()\n",
    "        print(\"ğŸš€ All embeddings successfully updated!\")\n",
    "\n",
    "    # Close connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "except psycopg2.OperationalError as e:\n",
    "    print(\"ğŸš¨ Connection failed! Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add JSON embeddings to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ğŸ”¥ Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… Retrieve PostgreSQL credentials\n",
    "dbname = os.getenv(\"NEON_DBNAME\", \"neondb\")\n",
    "user = os.getenv(\"NEON_USER\", \"neondb_owner\")\n",
    "password = os.getenv(\"NEON_PASSWORD\")\n",
    "host = os.getenv(\"NEON_HOST\", \"ep-morning-bonus-a8s0mhpb-pooler.eastus2.azure.neon.tech\")\n",
    "port = os.getenv(\"NEON_PORT\", \"5432\")\n",
    "\n",
    "# ğŸ”¥ Encode password\n",
    "encoded_password = quote_plus(password)\n",
    "\n",
    "# âœ… Construct PostgreSQL connection string\n",
    "conn_str = f\"postgresql://{user}:{encoded_password}@{host}:{port}/{dbname}?sslmode=require\"\n",
    "\n",
    "# âœ… Load embedding model\n",
    "MODEL_NAME = \"BAAI/bge-base-en\"\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# âœ… Connect to PostgreSQL\n",
    "conn = psycopg2.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "print(\"âœ… Connected to PostgreSQL!\")\n",
    "\n",
    "# âœ… Retrieve Pinecone API key\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "\n",
    "print(f\"âœ… Created new Pinecone index: {INDEX_NAME}\")\n",
    "\n",
    "# âœ… Connect to Pinecone index\n",
    "index = pc.Index(INDEX_NAME)\n",
    "print(f\"âœ… Connected to fresh Pinecone index: {INDEX_NAME}\")\n",
    "\n",
    "# âœ… Delete PostgreSQL tables and start fresh\n",
    "print(\"âš ï¸ Dropping and recreating PostgreSQL tables...\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS pinecone_log;\")\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE pinecone_log (\n",
    "        sep_id INTEGER PRIMARY KEY\n",
    "    );\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "print(\"âœ… PostgreSQL tables reset.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track errors\n",
    "error_entries = []\n",
    "last_processed_id = None  # Keep track of the last processed ID\n",
    "\n",
    "# 1ï¸âƒ£ Fetch JSON entries from PostgreSQL\n",
    "cursor.execute(\"SELECT id, title, mistral_output FROM sep_embeddings ORDER BY id;\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "batch_count = 0\n",
    "BATCH_SIZE = 100\n",
    "UPSERT_THRESHOLD = 100  # âœ… Force an upsert every 100 items\n",
    "\n",
    "while rows:\n",
    "    print(f\"âš™ï¸ Processing {len(rows)} entries in batch {batch_count + 1}...\")\n",
    "\n",
    "    upserts = []  # Collect Pinecone upserts\n",
    "\n",
    "    for text_id, document_title, mistral_json in rows:\n",
    "        last_processed_id = text_id  # Track last processed ID\n",
    "        try:\n",
    "            print(f\"\\nğŸ” Processing ID {text_id} - Raw Mistral JSON Type: {type(mistral_json)}\")\n",
    "\n",
    "            # Ensure mistral_json is a valid dictionary\n",
    "            if isinstance(mistral_json, str):\n",
    "                try:\n",
    "                    mistral_json = json.loads(mistral_json)\n",
    "                except json.JSONDecodeError:\n",
    "                    raise ValueError(\"Invalid JSON format\")\n",
    "\n",
    "            if not isinstance(mistral_json, dict):\n",
    "                raise ValueError(f\"Expected dictionary, got {type(mistral_json)}\")\n",
    "\n",
    "            # Process beliefs\n",
    "            for belief in mistral_json.get(\"key_beliefs\", []):\n",
    "                if not isinstance(belief, dict):\n",
    "                    print(f\"âš ï¸ Skipping belief: Expected dict, got {type(belief)}\")\n",
    "                    continue\n",
    "\n",
    "                belief_text = belief.get(\"belief\", \"\").strip()\n",
    "                justification = belief.get(\"justification\", \"\").strip()\n",
    "                related_concepts = \", \".join(c.get(\"name\", \"\").strip() for c in belief.get(\"related_concepts\", []) if isinstance(c, dict))\n",
    "\n",
    "                if not belief_text:\n",
    "                    print(\"âš ï¸ Skipping belief: Text is missing or empty\")\n",
    "                    continue\n",
    "\n",
    "                # Generate vector ID\n",
    "                vector_id = f\"{text_id}_belief_{hash(belief_text) % 10**8}\"\n",
    "\n",
    "                print(f\"ğŸŸ¢ Belief: {belief_text} (Generating Embedding...)\")\n",
    "\n",
    "                # Generate embeddings\n",
    "                combined_text = f\"{belief_text}. Justification: {justification}. Related concepts: {related_concepts}\"\n",
    "                belief_embedding = model.encode(combined_text).tolist()\n",
    "\n",
    "                if not belief_embedding:\n",
    "                    print(f\"âŒ ERROR: Empty embedding for belief `{belief_text}` (ID {text_id})\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"âœ… Embedding Generated: {belief_embedding[:5]}...\")  # Print first 5 dimensions for sanity check\n",
    "\n",
    "                # âœ… Add belief to upserts\n",
    "                upserts.append((\n",
    "                    vector_id,\n",
    "                    belief_embedding,\n",
    "                    {\n",
    "                        \"sep_id\": text_id,\n",
    "                        \"document_title\": document_title,\n",
    "                        \"type\": \"belief\",\n",
    "                        \"belief\": belief_text,\n",
    "                        \"justification\": justification,\n",
    "                        \"related_concepts\": related_concepts\n",
    "                    }\n",
    "                ))\n",
    "\n",
    "                print(f\"ğŸ”¹ Added belief `{belief_text}` to upserts! ({len(upserts)} total)\")\n",
    "\n",
    "            # Process key concepts\n",
    "            for concept in mistral_json.get(\"key_concepts\", []):\n",
    "                if not isinstance(concept, dict):\n",
    "                    print(f\"âš ï¸ Skipping concept: Expected dict, got {type(concept)}\")\n",
    "                    continue\n",
    "\n",
    "                concept_text = concept.get(\"name\", \"\").strip()\n",
    "\n",
    "                if not concept_text:\n",
    "                    print(\"âš ï¸ Skipping concept: Text is missing or empty\")\n",
    "                    continue\n",
    "\n",
    "                # Generate vector ID\n",
    "                vector_id = f\"{text_id}_concept_{hash(concept_text) % 10**8}\"\n",
    "\n",
    "                print(f\"ğŸ”´ Concept: {concept_text} (Generating Embedding...)\")\n",
    "\n",
    "                # Generate embeddings\n",
    "                concept_embedding = model.encode(concept_text).tolist()\n",
    "\n",
    "                if not concept_embedding:\n",
    "                    print(f\"âŒ ERROR: Empty embedding for concept `{concept_text}` (ID {text_id})\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"âœ… Embedding Generated: {concept_embedding[:5]}...\")  # Print first 5 dimensions for sanity check\n",
    "\n",
    "                # âœ… Add concept to upserts\n",
    "                upserts.append((\n",
    "                    vector_id,\n",
    "                    concept_embedding,\n",
    "                    {\n",
    "                        \"sep_id\": text_id,\n",
    "                        \"document_title\": document_title,\n",
    "                        \"type\": \"concept\",\n",
    "                        \"concept\": concept_text\n",
    "                    }\n",
    "                ))\n",
    "\n",
    "                print(f\"ğŸ”¹ Added concept `{concept_text}` to upserts! ({len(upserts)} total)\")\n",
    "\n",
    "            # âœ… ğŸš¨ **Trigger Upsert Every 100 Items** ğŸš¨ âœ…\n",
    "            if len(upserts) >= UPSERT_THRESHOLD:\n",
    "                print(f\"ğŸš€ ğŸ”„ Upserting {len(upserts)} items into Pinecone...\")\n",
    "\n",
    "                try:\n",
    "                    response = index.upsert(upserts)\n",
    "                    print(f\"ğŸ”„ Pinecone Upsert Response: {response}\")\n",
    "\n",
    "                    index_stats = index.describe_index_stats()\n",
    "                    print(f\"ğŸ“Š Pinecone Index Stats After Upsert: {index_stats}\")\n",
    "\n",
    "                    if index_stats[\"total_vector_count\"] == 0:\n",
    "                        print(\"ğŸš¨ ERROR: Pinecone is not storing the embeddings!\")\n",
    "                    else:\n",
    "                        print(\"âœ… Successfully inserted batch into Pinecone!\")\n",
    "\n",
    "                    upserts.clear()  # âœ… Clear upserts after inserting\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"ğŸš¨ ERROR! Pinecone upsert failed: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nğŸš¨ ERROR! Skipping ID {text_id} due to error:\\n{e}\")\n",
    "            error_entries.append(text_id)\n",
    "            continue  # Skip to next entry\n",
    "\n",
    "    # **Final Batch Upsert**\n",
    "    if upserts:\n",
    "        print(f\"ğŸš€ ğŸ”„ FINAL Upserting {len(upserts)} remaining items into Pinecone...\")\n",
    "\n",
    "        try:\n",
    "            response = index.upsert(upserts)\n",
    "            print(f\"ğŸ”„ Pinecone Upsert Response: {response}\")\n",
    "\n",
    "            index_stats = index.describe_index_stats()\n",
    "            print(f\"ğŸ“Š Pinecone Index Stats After Final Upsert: {index_stats}\")\n",
    "\n",
    "            upserts.clear()  # âœ… Clear upserts after inserting\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ğŸš¨ ERROR! Final Pinecone upsert failed: {e}\")\n",
    "\n",
    "    batch_count += 1\n",
    "    print(f\"ğŸš€ Committed batch {batch_count}\")\n",
    "\n",
    "    # Fetch next batch\n",
    "    cursor.execute(\"SELECT id, title, mistral_output FROM sep_embeddings ORDER BY id LIMIT 100 OFFSET %s;\", (batch_count * BATCH_SIZE,))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "# âœ… Close PostgreSQL connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"ğŸš€ All embeddings successfully updated in Pinecone!\")\n",
    "\n",
    "# Print error report\n",
    "if error_entries:\n",
    "    print(\"\\nâš ï¸ The following entries failed to process:\")\n",
    "    print(f\"ğŸ›‘ {len(error_entries)} error entries.\")\n",
    "    print(error_entries)\n",
    "\n",
    "# âœ… Print last processed JSON ID before exit\n",
    "if last_processed_id:\n",
    "    print(f\"\\nğŸ”„ Last successfully processed JSON ID before exit: {last_processed_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# ğŸ”¥ Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… Retrieve Pinecone API key\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# âœ… Create Pinecone instance\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# âœ… Define Index Name\n",
    "INDEX_NAME = \"belief-embeddings\"  # Change this if necessary\n",
    "\n",
    "# âœ… Connect to the Pinecone index\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# âœ… Get index statistics\n",
    "stats = index.describe_index_stats()\n",
    "\n",
    "# âœ… Print the number of entries\n",
    "num_entries = stats[\"total_vector_count\"]\n",
    "print(f\"Total number of entries in Pinecone index '{INDEX_NAME}': {num_entries}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
