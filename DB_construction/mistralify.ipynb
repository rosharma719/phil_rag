{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the JSONs in the Postgres and extract valuable information. This is the main knowledge transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 API Keys Loaded:\n",
      "MISTRAL_API_KEY_1: ptbzbx\n",
      "MISTRAL_API_KEY_2: pGxUxC\n",
      "MISTRAL_API_KEY_3: m9WQW8\n",
      "✅ Successfully connected to PostgreSQL!\n",
      "📊 Total unprocessed entries: 34723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 121/34828 [07:23<35:21:42,  3.67s/entry]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import psycopg2\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from dotenv import load_dotenv\n",
    "from queue import Queue\n",
    "\n",
    "# ✅ Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# ✅ Ensure required environment variables are loaded\n",
    "REQUIRED_ENV_VARS = [\"MISTRAL_API_KEY\", \"MISTRAL_API_KEY_2\", \"POSTGRES_PASSWORD\", \"POSTGRES_DB\"]\n",
    "for var in REQUIRED_ENV_VARS:\n",
    "    if not os.getenv(var):\n",
    "        raise EnvironmentError(f\"❌ Missing required environment variable: {var}\")\n",
    "\n",
    "MISTRAL_API_KEY_1 = os.getenv(\"MISTRAL_API_KEY\")  # First API key\n",
    "MISTRAL_API_KEY_2 = os.getenv(\"MISTRAL_API_KEY_2\")  # Second API key\n",
    "MISTRAL_API_KEY_3 = os.getenv(\"MISTRAL_API_KEY_3\")  # Third API key\n",
    "BATCH_SIZE = 10  # Upload to PostgreSQL every 10 processed entries\n",
    "\n",
    "print(\"🔑 API Keys Loaded:\")\n",
    "print(f\"MISTRAL_API_KEY_1: {MISTRAL_API_KEY_1[-6:]}\")\n",
    "print(f\"MISTRAL_API_KEY_2: {MISTRAL_API_KEY_2[-6:]}\")\n",
    "print(f\"MISTRAL_API_KEY_3: {MISTRAL_API_KEY_3[-6:]}\")\n",
    "\n",
    "\n",
    "# ✅ PostgreSQL Connection\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=os.getenv(\"POSTGRES_DB\"),\n",
    "        user=\"rohansharma\",\n",
    "        password=os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    print(\"✅ Successfully connected to PostgreSQL!\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"❌ Failed to connect to PostgreSQL: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ✅ Count total unprocessed entries\n",
    "try:\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM sep_embeddings WHERE mistral_output IS NULL;\")\n",
    "    total_entries = cursor.fetchone()[0]\n",
    "    print(f\"📊 Total unprocessed entries: {total_entries}\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"❌ Error executing query: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ✅ Fetch unprocessed rows\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT id, content FROM sep_embeddings \n",
    "    WHERE mistral_output IS NULL \n",
    "    ORDER BY id;\n",
    "\"\"\")\n",
    "rows = cursor.fetchall()\n",
    "batch_updates = []\n",
    "\n",
    "# ✅ Initialize progress bar\n",
    "progress_bar = tqdm(total=total_entries, desc=\"Processing Entries\", unit=\"entry\")\n",
    "\n",
    "# ✅ Separate queues for each API key\n",
    "queue_1 = Queue()\n",
    "queue_2 = Queue()\n",
    "queue_3 = Queue()\n",
    "response_queue = Queue()\n",
    "\n",
    "# ✅ Distribute requests between the three API keys\n",
    "for i, (entry_id, content) in enumerate(rows, start=1):\n",
    "    if i % 3 == 0:\n",
    "        queue_1.put((entry_id, content))  # ✅ Every third entry → API Key 1\n",
    "    elif i % 3 == 1:\n",
    "        queue_2.put((entry_id, content))  # ✅ Every third entry → API Key 2\n",
    "    else:\n",
    "        queue_3.put((entry_id, content))  # ✅ Every third entry → API Key 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from queue import Empty\n",
    "\n",
    "# ✅ Locks to enforce **1 RPS per key**\n",
    "api_lock_1 = threading.Lock()\n",
    "api_lock_2 = threading.Lock()\n",
    "api_lock_3 = threading.Lock()\n",
    "\n",
    "def call_mistral_api(api_key, lock, entry_id, content, attempt=0):\n",
    "    \"\"\" Sends an API request with exponential backoff for 429 errors. \"\"\"\n",
    "    url = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Extract structured philosophical knowledge. Your response should always be a single valid JSON object.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        You are an AI tasked with extracting structured philosophical knowledge from a given text.\n",
    "        Your goal is to **categorize the entry and extract key information in a structured format.**  \n",
    "        Be **as detailed as possible while remaining concise.**  \n",
    "        **Always return a single JSON object, never multiple JSON objects.**\n",
    "\n",
    "        ## **Input:**\n",
    "        {content}\n",
    "\n",
    "        ## **Output Format (JSON):**\n",
    "        {{\n",
    "            \"category\": \"thinker\" | \"concept\" | \"era\",\n",
    "            \"metadata\": {{\n",
    "                \"name\": \"...\", \n",
    "                \"description\": \"...\",\n",
    "                \"time_period\": \"...\"  \n",
    "            }},\n",
    "            \"key_beliefs\": [  \n",
    "                {{ \"belief\": \"...\", \"justification\": \"...\", \"related_concepts\": [\"...\", \"...\"] }}\n",
    "            ],\n",
    "            \"key_concepts\": [  \n",
    "                {{ \"name\": \"...\", \"definition\": \"...\", \"related_fields\": [\"...\", \"...\"] }}\n",
    "            ],\n",
    "            \"associated_thinkers\": [\"...\", \"...\"],\n",
    "            \"associated_eras\": [\"...\", \"...\"]\n",
    "        }}\n",
    "        **Never return multiple JSON objects.**\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"mistral-medium-2312\",\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.4\n",
    "    }\n",
    "\n",
    "    with lock:  # ✅ Ensures strict **1 request per second per key**\n",
    "        time.sleep(1.1)  # ✅ Strict rate limit enforcement\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            parsed_response = response.json()\n",
    "            return entry_id, json.loads(parsed_response[\"choices\"][0][\"message\"][\"content\"])  # ✅ Ensure Valid JSON\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"❌ Failed to parse API response for entry ID {entry_id}.\")\n",
    "            return None\n",
    "\n",
    "    elif response.status_code == 429:\n",
    "        if attempt < 5:  # 🚀 Retry up to 5 times with exponential backoff\n",
    "            wait_time = 2 ** attempt  # Exponential backoff: 2s → 4s → 8s → 16s → 32s\n",
    "            print(f\"\\n⏳ Rate limited (429) for entry {entry_id}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            return call_mistral_api(api_key, lock, entry_id, content, attempt + 1)\n",
    "        else:\n",
    "            print(f\"\\n❌ Entry {entry_id} failed after 5 retries. Skipping...\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        print(f\"\\n❌ API request failed for entry ID {entry_id}: {response.status_code} {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 8/34723 [00:20<22:02:44,  2.29s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Current batch size: 1 | ⬆️ Processing entry 210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 10/34723 [00:28<29:49:44,  3.09s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed entry 118 with bzbx\n",
      "📦 Current batch size: 2 | ⬆️ Processing entry 118\n",
      "✅ Processed entry 115 with WQW8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 12/34723 [00:29<19:23:08,  2.01s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Current batch size: 3 | ⬆️ Processing entry 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 14/34723 [00:31<13:46:11,  1.43s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Current batch size: 4 | ⬆️ Processing entry 239\n",
      "✅ Processed entry 112 with xUxC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 16/34723 [00:40<29:47:59,  3.09s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed entry 124 with WQW8\n",
      "📦 Current batch size: 5 | ⬆️ Processing entry 124\n",
      "✅ Processed entry 127 with bzbx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 18/34723 [00:43<23:18:05,  2.42s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed entry 121 with xUxC\n",
      "📦 Current batch size: 6 | ⬆️ Processing entry 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 20/34723 [00:51<32:32:01,  3.37s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Current batch size: 7 | ⬆️ Processing entry 212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 22/34723 [00:54<23:03:44,  2.39s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed entry 130 with xUxC\n",
      "📦 Current batch size: 8 | ⬆️ Processing entry 130\n",
      "✅ Processed entry 133 with WQW8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 24/34723 [00:57<20:06:56,  2.09s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed entry 136 with bzbx\n",
      "📦 Current batch size: 9 | ⬆️ Processing entry 136\n",
      "\n",
      "⏳ Rate limited (429) for entry 145. Retrying in 1 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 26/34723 [01:02<23:31:38,  2.44s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Current batch size: 10 | ⬆️ Processing entry 215\n",
      "\n",
      "✅ Uploaded 10 entries to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 27/34723 [01:09<34:44:11,  3.60s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Current batch size: 1 | ⬆️ Processing entry 254\n",
      "✅ Processed entry 142 with WQW8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries:   0%|          | 29/34723 [01:10<21:11:05,  2.20s/entry]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed entry 139 with xUxC\n",
      "📦 Current batch size: 2 | ⬆️ Processing entry 139\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import json\n",
    "import time\n",
    "import psycopg2\n",
    "from queue import Queue, Empty\n",
    "\n",
    "# ✅ Define API worker function\n",
    "def api_worker(queue, api_key, lock):\n",
    "    \"\"\" Worker that fetches requests from the queue, calls the API, and puts results into the response queue. \"\"\"\n",
    "    while not queue.empty():\n",
    "        try:\n",
    "            entry_id, content = queue.get(timeout=10)\n",
    "            response = call_mistral_api(api_key, lock, entry_id, content)\n",
    "            if response:\n",
    "                response_queue.put(response)  # ✅ Add only successful responses\n",
    "                print(f\"✅ Processed entry {entry_id} with {api_key[-4:]}\")\n",
    "            queue.task_done()\n",
    "\n",
    "        except Empty:\n",
    "            break\n",
    "\n",
    "\n",
    "# ✅ Define response processing function\n",
    "def process_responses():\n",
    "    \"\"\" Processes API responses and updates the database in batches. \"\"\"\n",
    "    batch_updates = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            entry_id, mistral_output_json = response_queue.get(timeout=10)\n",
    "            batch_updates.append((json.dumps(mistral_output_json), entry_id))\n",
    "            progress_bar.update(1)\n",
    "            print(f\"📦 Current batch size: {len(batch_updates)} | ⬆️ Processing entry {entry_id}\")\n",
    "\n",
    "            # ✅ Upload batch every 10 responses\n",
    "            if len(batch_updates) >= BATCH_SIZE:\n",
    "                cursor.executemany(\"\"\"\n",
    "                    UPDATE sep_embeddings \n",
    "                    SET mistral_output = %s \n",
    "                    WHERE id = %s;\n",
    "                \"\"\", batch_updates)\n",
    "                conn.commit()\n",
    "                print(f\"\\n✅ Uploaded {BATCH_SIZE} entries to the database.\")\n",
    "                batch_updates = []  # ✅ Reset batch\n",
    "\n",
    "        except Empty:\n",
    "            # ✅ Stop processing **only when all API workers have finished**\n",
    "            if queue_1.empty() and queue_2.empty() and queue_3.empty() and response_queue.empty():\n",
    "                break  # ✅ Exit if everything is processed\n",
    "\n",
    "    # ✅ Upload any remaining entries\n",
    "    if batch_updates:\n",
    "        cursor.executemany(\"\"\"\n",
    "            UPDATE sep_embeddings \n",
    "            SET mistral_output = %s \n",
    "            WHERE id = %s;\n",
    "        \"\"\", batch_updates)\n",
    "        conn.commit()\n",
    "        print(f\"\\n✅ Final batch uploaded successfully ({len(batch_updates)} entries).\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    print(\"\\n🎉 Processing complete!\")\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# ✅ Start API worker threads (3 keys)\n",
    "api_thread_1 = threading.Thread(target=api_worker, args=(queue_1, MISTRAL_API_KEY_1, api_lock_1))\n",
    "api_thread_2 = threading.Thread(target=api_worker, args=(queue_2, MISTRAL_API_KEY_2, api_lock_2))\n",
    "api_thread_3 = threading.Thread(target=api_worker, args=(queue_3, MISTRAL_API_KEY_3, api_lock_3))\n",
    "\n",
    "response_thread = threading.Thread(target=process_responses)\n",
    "\n",
    "# ✅ Start all threads\n",
    "api_thread_1.start()\n",
    "api_thread_2.start()\n",
    "api_thread_3.start()\n",
    "response_thread.start()\n",
    "\n",
    "# ✅ Wait for API workers to complete first\n",
    "api_thread_1.join()\n",
    "api_thread_2.join()\n",
    "api_thread_3.join()\n",
    "\n",
    "# ✅ Ensure all responses are processed before exiting\n",
    "response_thread.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reupload this data to the Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and upload the vector embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
